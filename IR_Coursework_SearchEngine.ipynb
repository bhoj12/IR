{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-ratio",
   "metadata": {},
   "source": [
    "# Information Retrieval Coursework (7071CEM)\n",
    "\n",
    "Task:\n",
    "Develop a vertical search engine similar to Google Scholar that only retrieves papers/books published by a member of Coventry University. As such, at least one of the co-authors must be a Coventry University Staff member. \n",
    "\n",
    "To this end, the profiles of academic staff at CU available on the CU web site are crawled, and their papers within their profiles are indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-simple",
   "metadata": {},
   "source": [
    "## Package Installs\n",
    "\n",
    "Some packages required for this task are by default not included in the jupyter notebooks package list. A pip-install of these packages is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-guatemala",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:01.977307Z",
     "start_time": "2021-08-01T00:58:01.962437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the \"#\" before each subsequent line to install the package\n",
    "\n",
    "#!pip install scrapy\n",
    "#!pip install requests\n",
    "#!pip install BeautifulSoup4\n",
    "#!pip install nltk\n",
    "#!pip install gensim\n",
    "#!pip install xgboost\n",
    "#!pip install pandastable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-registration",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-pioneer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:06.417024Z",
     "start_time": "2021-08-01T00:58:01.980987Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-capitol",
   "metadata": {},
   "source": [
    "# 1. Crawler Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-marks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:06.423826Z",
     "start_time": "2021-08-01T00:58:06.420176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Seed Page\n",
    "URL = \"https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/\"\n",
    "\n",
    "# Define profile URL format. This was obtained by manually examining the profile pages\n",
    "profile_url = \"https://pureportal.coventry.ac.uk/en/persons/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-repeat",
   "metadata": {},
   "source": [
    "The seed page, i.e coventry researchers profiles landing page, contains profiles of all academic researchers of the University. As there are up to 2206 results, a limited number are displayed at once (50).\n",
    "\n",
    "Even if new profiles are added and a new page is required, the crawler will need to crawl through all pages to access every profile. To do this, a function is defined to retrieve the current total number of result pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-shift",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:09.277766Z",
     "start_time": "2021-08-01T00:58:06.428114Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_maximum_page():\n",
    "    \n",
    "    first = requests.get(URL)\n",
    "    soup = BeautifulSoup(first.text, 'html.parser')\n",
    "    final_page = soup.select('#main-content > div > section > nav > ul > li:nth-child(12) > a')[0]['href']\n",
    "    fp = final_page.split('=')[-1]\n",
    "    return int(fp)\n",
    "    \n",
    "mx = get_maximum_page()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-mustang",
   "metadata": {},
   "source": [
    "Instead of crawling all researchers, this web crawler is designed to specifically find researchers who:\n",
    "    \n",
    "1. Have research publications\n",
    "2. Are part of the \"**School of Computing, Electronics and Maths**\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-sugar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:09.407285Z",
     "start_time": "2021-08-01T00:58:09.280073Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_department(researcher):\n",
    "    \n",
    "    l1 = researcher.find('div', class_='rendering_person_short')\n",
    "      \n",
    "    for span in l1.find_all('span'):\n",
    "        # Check department\n",
    "        if span.text == str('School of Computing, Electronics and Maths'):\n",
    "            name = researcher.find('h3', class_='title').find('span').text\n",
    "            return name\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def create_csv():\n",
    "    database = pd.DataFrame(columns=['Title', 'Author', 'Published', 'Link'])\n",
    "    database.to_csv('database.csv')\n",
    "    \n",
    "def update_csv(database):\n",
    "    current_data = pd.read_csv(database, index_col=\"Unnamed: 0\")\n",
    "    return current_data        \n",
    "\n",
    "def enter_each_researchers_publication(researcher, url, df):\n",
    "    \n",
    "    new_url = url + str(researcher).replace(' ','-').lower() + '/publications/'\n",
    "    page = requests.get(new_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(id=\"main-content\")\n",
    "    papers = results.find_all(\"li\", class_=\"list-result-item\")\n",
    "    \n",
    "    \n",
    "    for paper in papers:\n",
    "        title = paper.find('h3', class_='title').find('span')\n",
    "        author = paper.find('a', class_='link person').find('span')\n",
    "        date = paper.find('span', class_=\"date\")\n",
    "        link = paper.find('h3', class_='title').find('a', href=True)['href']\n",
    "        \n",
    "        opening = pd.read_csv('database.csv', index_col=\"Unnamed: 0\")\n",
    "        opening = opening.append({'Title': title.text, \n",
    "                                  'Author': author.text, \n",
    "                                  'Published': date.text,\n",
    "                                  'Link': link}, ignore_index=True)\n",
    "        opening.to_csv('database.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-customer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:09.442618Z",
     "start_time": "2021-08-01T00:58:09.431490Z"
    }
   },
   "outputs": [],
   "source": [
    "## Scrape function\n",
    "def scrape(mx):\n",
    "    df = update_csv('database.csv')\n",
    "    i=0\n",
    "    while True:\n",
    "    \n",
    "        if i > 17:\n",
    "            break\n",
    "            \n",
    "        if i>0:\n",
    "            url = URL + '?page=' + str(i)\n",
    "        else:\n",
    "            url = URL\n",
    "    \n",
    "        i = i+1\n",
    "        # scraping starts here\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(id=\"main-content\")\n",
    "        researchers = results.find_all(\"li\", class_=\"grid-result-item\")\n",
    "\n",
    "        for researcher in researchers:\n",
    "            # Check if researcher has any papers\n",
    "            check = researcher.find('div', class_='stacked-trend-widget')\n",
    "            if check:\n",
    "                name = check_department(researcher)\n",
    "                if name is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    enter_each_researchers_publication(name, profile_url, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-vegetable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.554457Z",
     "start_time": "2021-08-01T00:58:09.447859Z"
    }
   },
   "outputs": [],
   "source": [
    "create_csv()\n",
    "\n",
    "%time scrape(mx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-subscription",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.569218Z",
     "start_time": "2021-08-01T00:59:40.557023Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_db = pd.read_csv('database.csv').rename(columns={'Unnamed: 0':'SN'})\n",
    "sample_db\n",
    "print(f'{sample_db.shape[0]} records were scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-desktop",
   "metadata": {},
   "source": [
    "# 2. Indexing Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-potter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.600384Z",
     "start_time": "2021-08-01T00:59:40.572032Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_db = pd.read_csv('database.csv').rename(columns={'Unnamed: 0':'SN'}).reset_index(drop=True)\n",
    "scraped_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-coordinate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T07:22:25.795248Z",
     "start_time": "2021-08-01T07:22:25.784819Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_db.head(7)\n",
    "#ids = scraped_db[\"Title\"]\n",
    "#scraped_db[ids.isin(ids[ids.duplicated()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-trauma",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.612357Z",
     "start_time": "2021-08-01T00:59:40.603287Z"
    }
   },
   "outputs": [],
   "source": [
    "single_row = scraped_db.loc[1,:].copy()\n",
    "single_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-humor",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-train",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.647183Z",
     "start_time": "2021-08-01T00:59:40.615161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "sw = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tp1(txt):\n",
    "    txt = txt.lower()   # Make lowercase\n",
    "    txt = txt.translate(str.maketrans('',\n",
    "                                      '',\n",
    "                                      string.punctuation))   # Remove punctuation marks\n",
    "    txt = lematize(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def fwpt(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    hash_tag = {\"V\": wordnet.VERB, \"R\": wordnet.ADV,\"N\": wordnet.NOUN,\"J\": wordnet.ADJ}         \n",
    "    return hash_tag.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lematize(text):\n",
    "        tkns = nltk.word_tokenize(text)\n",
    "        ax = \"\"\n",
    "        for each in tkns:\n",
    "            if each not in sw:\n",
    "                ax += lemmatizer.lemmatize(each, fwpt(each)) + \" \"\n",
    "        return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-triple",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.670745Z",
     "start_time": "2021-08-01T00:59:40.658588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample title\n",
    "single_row['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-strain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.014244Z",
     "start_time": "2021-08-01T00:59:40.699046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstration of lowercase and punctuation removal\n",
    "tp1(single_row['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-columbus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.027827Z",
     "start_time": "2021-08-01T00:59:43.016665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstration of lematization\n",
    "\n",
    "lematize(tp1(single_row['Title']))\n",
    "#lematize(single_row['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-retrieval",
   "metadata": {},
   "source": [
    "#### Unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-snapshot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T07:25:01.875323Z",
     "start_time": "2021-08-01T07:25:01.858252Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_db['Title'].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-height",
   "metadata": {},
   "source": [
    "#### Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-burton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T07:25:27.562359Z",
     "start_time": "2021-08-01T07:25:27.557862Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_db['Title'].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-level",
   "metadata": {},
   "source": [
    "### 2.1.1 Preprocess entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-alexander",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.834681Z",
     "start_time": "2021-08-01T00:59:43.030064Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_db = scraped_db.copy()\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df.Title = df.Title.apply(tp1)\n",
    "    df.Author = df.Author.str.lower()\n",
    "    df = df.drop(columns=['Author','Published'], axis=1)\n",
    "    return df\n",
    "    \n",
    "preprocess_df(processed_db)\n",
    "processed_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-driver",
   "metadata": {},
   "source": [
    "## 2.2 Index Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-water",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.845672Z",
     "start_time": "2021-08-01T00:59:43.837012Z"
    }
   },
   "outputs": [],
   "source": [
    "single = processed_db.loc[0,:].copy()\n",
    "print(single)\n",
    "indexing_trial = {}\n",
    "\n",
    "words = single.Title.split()\n",
    "SN = single.SN\n",
    "word = words[0]\n",
    "example = {word: [SN]}\n",
    "\n",
    "print('=====================================================================')\n",
    "print('Sample index')\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-flour",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.854491Z",
     "start_time": "2021-08-01T00:59:43.848459Z"
    }
   },
   "outputs": [],
   "source": [
    "## Indexer Function\n",
    "def apply_index(inputs, index):\n",
    "    words = inputs.Title.split()\n",
    "    SN = int(inputs.SN)\n",
    "    for word in words:\n",
    "        if word in index.keys():\n",
    "            if SN not in index[word]:\n",
    "                index[word].append(SN)\n",
    "        else:\n",
    "            index[word] = [SN]\n",
    "    return index\n",
    "\n",
    "indx = apply_index(inputs=single, index= {})\n",
    "#print(indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-senior",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T02:46:26.572666Z",
     "start_time": "2021-07-31T02:46:26.567533Z"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-amsterdam",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:44.790404Z",
     "start_time": "2021-08-01T00:59:43.857267Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_index(df, index):\n",
    "    for x in range(len(df)):\n",
    "        inpt = df.loc[x,:]\n",
    "        ind = apply_index(inputs=inpt, index=index)\n",
    "    return ind\n",
    "\n",
    "def construct_index(df, index):\n",
    "    queue = preprocess_df(df)\n",
    "    ind = full_index(df=queue, index=index)\n",
    "    return ind\n",
    "\n",
    "indexed = full_index(processed_db, \n",
    "                     index = {})\n",
    "\n",
    "\n",
    "indexes = construct_index(df=scraped_db, \n",
    "                          index = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-piano",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:44.827243Z",
     "start_time": "2021-08-01T00:59:44.796816Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('indexes.json', 'w') as new_f:\n",
    "    json.dump(indexes, new_f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open('indexes.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def index_2(df, x_path):\n",
    "    if len(df) > 0:\n",
    "        with open(x_path, 'r') as file:\n",
    "            prior_index = json.load(file)\n",
    "        new_index = construct_index(df = df, index = prior_index)\n",
    "        with open(x_path, 'w') as new_f:\n",
    "            json.dump(new_index, new_f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-jesus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T10:41:54.049783Z",
     "start_time": "2021-08-01T10:41:53.953783Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-editing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T23:13:44.430869Z",
     "start_time": "2021-08-01T23:13:44.285366Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-paintball",
   "metadata": {},
   "source": [
    "## 3.  Query Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-football",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:02:51.119226Z",
     "start_time": "2021-08-01T11:02:51.112657Z"
    }
   },
   "outputs": [],
   "source": [
    "def demonstrate_query_processing():\n",
    "    sample = input('Enter Search Terms: ')\n",
    "    processed_query = tp1(sample)\n",
    "    #print(f'User Search Query: {sample}')\n",
    "    print(f'Processed Search Query: {processed_query}')\n",
    "    return processed_query\n",
    "    \n",
    "#demonstrate_query_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-turkish",
   "metadata": {},
   "source": [
    "### 3.1.  Split Query into individual terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-client",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:03:01.618023Z",
     "start_time": "2021-08-01T11:02:59.088852Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_query(terms):\n",
    "    each = tp1(terms)\n",
    "    return each.split()\n",
    "\n",
    "dqp = demonstrate_query_processing()\n",
    "dqp\n",
    "print(f'Split Query: {split_query(dqp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-postage",
   "metadata": {},
   "source": [
    "### 3.2.  Boolean Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-denial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:00:26.010007Z",
     "start_time": "2021-08-01T01:00:26.004950Z"
    }
   },
   "outputs": [],
   "source": [
    "def union(lists):\n",
    "    union = list(set.union(*map(set, lists)))\n",
    "    union.sort()\n",
    "    return union\n",
    "\n",
    "def intersection(lists):\n",
    "    intersect = list(set.intersection(*map(set, lists)))\n",
    "    intersect.sort()\n",
    "    return intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-distinction",
   "metadata": {},
   "source": [
    "### 3.3. Search Engine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-pottery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:00:26.020938Z",
     "start_time": "2021-08-01T01:00:26.012920Z"
    }
   },
   "outputs": [],
   "source": [
    "def vertical_search_engine(df, query, index=indexes):\n",
    "    query_split = split_query(query)\n",
    "    retrieved = []\n",
    "    for word in query_split:\n",
    "        if word in index.keys():\n",
    "            retrieved.append(index[word])\n",
    "            \n",
    "            \n",
    "    # Ranked Retrieval\n",
    "    if len(retrieved)>0:\n",
    "        high_rank_result = intersection(retrieved)\n",
    "        low_rank_result = union(retrieved) \n",
    "        c = [x for x in low_rank_result if x not in high_rank_result]      \n",
    "        high_rank_result.extend(c)\n",
    "        result = high_rank_result\n",
    "        \n",
    "        final_output = df[df.SN.isin(result)].reset_index(drop=True)\n",
    "    \n",
    "        # Return result in order of Intersection ----> Union\n",
    "        dummy = pd.Series(result, name = 'SN').to_frame()\n",
    "        result = pd.merge(dummy, final_output, on='SN', how = 'left')\n",
    "        \n",
    "    else:\n",
    "        result = 'No result found'\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-killer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:00:37.508501Z",
     "start_time": "2021-08-01T01:00:26.023723Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_search_engine():\n",
    "    xtest = scraped_db.copy()\n",
    "    query = input(\"Enter your search query: \")\n",
    "    return vertical_search_engine(xtest, query, indexed)\n",
    "    \n",
    "test_search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-crazy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:43:05.137930Z",
     "start_time": "2021-08-01T11:43:05.131660Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_engine(results):\n",
    "    if type(results) != 'list':\n",
    "        return results\n",
    "        #print(results)\n",
    "    else:\n",
    "        for i in range(len(results)):\n",
    "            printout = results.loc[i, :]\n",
    "            #print(printout['Title'])\n",
    "            #print(printout['Author'])\n",
    "            #print(printout['Published'])\n",
    "            #print(printout['Link'])\n",
    "            #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-astrology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:46:39.943876Z",
     "start_time": "2021-08-01T11:46:39.927460Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_db['Title'].iloc[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-savings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:44:03.697525Z",
     "start_time": "2021-08-01T11:43:54.760859Z"
    }
   },
   "outputs": [],
   "source": [
    "final_engine(test_search_engine())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-toilet",
   "metadata": {},
   "source": [
    "## 4. Schedule Crawler for every week\n",
    "\n",
    "To demonstrate a weekly scheduled crawling, the following parameters are defined:\n",
    "\n",
    "* `interval` : Represents number of days in reality. In this code, it represents only seconds for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-prayer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:05:57.578325Z",
     "start_time": "2021-08-01T01:00:37.523485Z"
    }
   },
   "outputs": [],
   "source": [
    "days = 0\n",
    "interval = 7\n",
    "while days <= 1:\n",
    "    scrape(mx)\n",
    "    print(f\"Crawled at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f'Next crawl scheduled after {interval} days')\n",
    "    time.sleep(interval)\n",
    "    days = days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "processed-movement",
   "metadata": {},
   "source": [
    "# 5. GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-victim",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:15:13.914499Z",
     "start_time": "2021-08-01T01:15:13.885800Z"
    }
   },
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from PIL import Image, ImageTk \n",
    "from tkinter import scrolledtext\n",
    "from pandastable import Table, TableModel\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "image1 = Image.open('coventry-university-logo.png')\n",
    "resized_image1 = image1.resize((500,300), Image.ANTIALIAS)\n",
    "\n",
    "def new_gui(pic):\n",
    "    window = Tk()\n",
    "    window.title(\"Coventry University Oracle\")\n",
    "    window.geometry('1100x600')\n",
    "    \n",
    "    lbl = Label(window, text=\"CEM Search Engine\", font=(\"Arial Bold\", 30), padx=5, pady=5)\n",
    "    lbl.grid(column=1, row=0)\n",
    "    \n",
    "    lbl2 = Label(window, text=\"Enter your search query here ===>\", font=(\"Arial\", 15), padx=5, pady=5)\n",
    "    lbl2.grid(column=0, row=1)\n",
    "    \n",
    "    \n",
    "    img = ImageTk.PhotoImage(pic)\n",
    "    \n",
    "    lbl3 = Label(image=img)\n",
    "    lbl3.image = img\n",
    "    lbl3.grid(column=1, row=3, padx=5, pady=5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    query = Entry(window,width=40)\n",
    "    query.grid(column=1, row=1,  padx=5, pady=5)\n",
    "    \n",
    "    results = Canvas(window, height=30, width=250)\n",
    "    results.grid(column=1, row=2, padx=5, pady=5)\n",
    "    \n",
    "    # Entry\n",
    "    def getInputBoxValue():\n",
    "        userInput = query.get()\n",
    "        return userInput\n",
    "\n",
    "    \n",
    "    # Button\n",
    "    def clicked():\n",
    "        search()\n",
    "        #pass\n",
    "        \n",
    "    def no_result():\n",
    "        messagebox.showwarning(\"Warning\", \"No results found. Please try different search terms\")\n",
    "        \n",
    "    \n",
    "    def search():\n",
    "        xtest = scraped_db.copy()\n",
    "        q = query.get()\n",
    "        f = Frame(window)\n",
    "        df = vertical_search_engine(xtest, q, indexed)\n",
    "        if type(df) == str:\n",
    "            no_result()\n",
    "        else:\n",
    "            pt = Table(results)\n",
    "            try:\n",
    "                table = pt = Table(results, dataframe=df)\n",
    "                pt.show()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            \n",
    "    def close_window():\n",
    "        if messagebox.askokcancel(\"Quit\", \"Quit Programme?\"):\n",
    "            window.destroy()\n",
    "        \n",
    "    \n",
    "    btn = Button(window, text=\"Search\", command=clicked)\n",
    "    btn.grid(column=2, row=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    window.protocol(\"WM_DELETE_WINDOW\", close_window)       \n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-spray",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T23:14:02.698938Z",
     "start_time": "2021-08-01T23:14:02.696390Z"
    }
   },
   "outputs": [],
   "source": [
    "new_gui(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-butterfly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
